<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>554e75aca2900445d84f7cd75aa81f0e13f6ce6e78fec171edf371295f2d2b18</job>
    <base_name>143g</base_name>
    <doi>http://dx.doi.org/10.7551/mitpress/9780262033589.003.0001</doi>
    <warning>Name identification was not possible. </warning>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <title-group>
        <article-title class="DoCO:Title" id="1">Introduction to Supervised Learning</article-title>
      </title-group>
      <region class="unknown" id="2">Erik G. Learned-Miller Department of Computer Science University of Massachusetts, Amherst Amherst, MA 01003 February 17, 2014</region>
      <abstract class="DoCO:Abstract" id="3">This document introduces the paradigm of supervised learning. It also discusses nearest neighbor classification and the distance functions nec- essary for nearest neighbor classification. It discusses Euclidean distance functions in two and three dimensions, and their extensions to higher dimensions, including distance functions between binary images, scalar- valued images, and 3-channel color images.</abstract>
      <outsider class="DoCO:TextBox" type="page_nr" id="4">1</outsider>
      <region class="unknown" id="5">1 Supervised learning</region>
      <outsider class="DoCO:TextBox" type="sidenote" id="6">supervised learning training set test set</outsider>
      <region class="DoCO:TextChunk" id="9">Supervised learning is simply a formalization of the idea of learning from examples. In supervised learning, the learner (typically, a computer program) is provided with two sets of data, a training set and a test set . The idea is for the learner to “learn” from a set of labeled examples in the training set so that it can identify unlabeled examples in the test set with the highest possible accuracy. That is, the goal of the learner is to develop a rule, a program, or a procedure that classifies new examples (in the test set) by analyzing examples it has been given that already have a class label. For example, a training set might consist of images of different types of fruit (say, peaches and nectarines), where the identity of the fruit in each image is given to the learner. The test set would then consist of more unidentified pieces of fruit, but from the same classes. The goal is for the learner to develop a rule that can identify the elements in the test set. There are many different approaches that attempt to build the best possible method of classifying examples of the test set by using the data given in the training set. We will discuss a few of these in this document, after defining supervised learning more formally. In supervised learning, the training set consists of n ordered pairs ( x 1 , y 1 ) , ( x 2 , y 2 ) , ..., ( x n , y n ), where each x i is some measurement or set of measurements of a single example data point, and y i is the label for that data point. For example, an x i might be a group (sometimes called a vector 1 ) of five measurements for a patient in a hospital including height, weight, temperature, blood sugar level, and blood pressure. The corresponding y i might be a classification of the patient as “healthy” or “not healthy”. The test data in supervised learning is another set of m measurements without labels: ( x n +1 , x n +2 , ..., x n + m ). As described above, the goal is to make educated guesses about the labels for the test set (such as “healthy” or “not healthy”) by drawing inferences from the training set. An example we will consider frequently in this book is when each x i is an image and the y i gives the class of the image. Suppose we are writing software to recognize types of fruit at a supermarket checkout counter so that the price could be entered automatically. To train such a classifier, we would provide sample images (a training set) for each type of fruit, as shown in <xref ref-type="fig" rid="F1" id="7" class="deo:Reference">Figure 1</xref>. Then we would use the classifier by having it label new images of fruit (a test set).<marker type="block"/> To get a feel for supervised learning, we will start by exploring one of the simplest algorithms that uses training data to help classify test data, the nearest neighbor rule or nearest neighbor algorithm. 1 A single group of measurements x i is often referred to as a vector of measurements, al- though a better term would probably be tuple , since the term vector implies certain properties that may not be true for a set of measurements.<marker type="page" number="3"/><marker type="block"/> 1.1.1 Nearest neighbor classification The nearest neighbor algorithm uses a simple intuitiion to classify test examples. Suppose that we have some way of computing a notion of distance between two observations x 1 and x 2 :</region>
      <region class="unknown" id="10">1.1 Getting started with supervised learning: Nearest neighbor algorithms</region>
      <outsider class="DoCO:TextBox" type="page_nr" id="11">2</outsider>
      <region class="DoCO:FigureBox" id="F1">
        <image class="DoCO:Figure" src="143g.page_003.image_01.png" thmb="143g.page_003.image_01-thumb.png"/>
        <caption class="deo:Caption" id="13">Figure 1: Training and test data for a fruit classifier. On the left are training data for the class of apples. In the middle are trainin data for the peaches class. On the right are a mixture of fruit which could be used to test the classifier.</caption>
      </region>
      <disp-formula class="DoCO:FormulaBox" id="F1">
        <label class="DoCO:Label" id="15">1</label>
        <content class="DoCO:Formula" id="16">D ( x 1 , x 2 ) .</content>
      </disp-formula>
      <region class="DoCO:TextChunk" id="18">We will talk about various options for the function D below, but for now you can think of it as a function which returns 0 when the data items x 1 and x 2 are exactly the same, and returns larger numbers as the two items become “more different”. Given a test data item x t the nearest neighbor classifier chooses the class corresponding to the data item in the training set with the lowest distance to the test example. Using the notation we established above, let i ∗ , the index of the training example closest (i.e., with minimum distance) to the test example x t , be defined as i ∗ = arg min D ( x t , x i ) . i ∈{ 1 ...n } After finding i ∗ , the nearest neighbor rule would assign to the test example the label y i ∗ , the label of the training example x i ∗ that was closest to the test example x t . Of course, this assignment of a label is merely a guess, and may be correct or incorrect. <marker type="block"/> 1.1.2 Distance functions To use the nearest neighbor classifier, we need to define the distance function D in Equation (1). In order to understand what a distance between images might look like, we start with a simpler notion of distance, the Euclidean distance in two dimensions. Let’s suppose we have two points p and q with</region>
      <outsider class="DoCO:TextBox" type="sidenote" id="19">Euclidean distance</outsider>
      <region class="DoCO:TextChunk" id="20" confidence="possible">p 1 p = p 2</region>
      <outsider class="DoCO:TextBox" type="page_nr" id="21">3</outsider>
      <region class="DoCO:TextChunk" id="22">and q 1 q = . q 2 p 1 , p 2 , q 1 , and q 2 are just the coordinates of p and q . These coordinates could represent spatial measurements along axes in a Euclidean plane, but they could also represent other measurements such as the height and weight of a patient. In general, we shall consider each coordinate to represent a particular measurement in a given experiment, and not restrict them to be spatial measurements. Whatever measurements these coordinates represent, the Euclidean distance between p and q is given by</region>
      <region class="DoCO:TextChunk" id="23" confidence="possible">D Euc ( p , q ) = ( p 1 − q 1 ) 2 + ( p 2 − q 2 ) 2 .</region>
      <region class="DoCO:TextChunk" id="24">Of course extending this formula to three spatial dimensions or three measurements is easy. If p and q consist of three measurements instead of two, we have D Euc ( p , q ) = ( p 1 − q 1 ) 2 + ( p 2 − q 2 ) 2 + ( p 3 − q 3 ) 2 . While it is harder to interpret using familiar 3-dimensional geometry, there is no reason we cannot extend this formula to p and q having four or more measurements. In general, when p and q consist of n measurements, we can define their n − dimensional Euclidean distance to be</region>
      <disp-formula class="DoCO:FormulaBox" id="F2">
        <label class="DoCO:Label" id="25">2</label>
        <content class="DoCO:Formula" id="26">D Euc ( p , q ) = ( p 1 − q 1 ) 2 + ( p 2 − q 2 ) 2 + ... + ( p n − q n ) 2 n</content>
      </disp-formula>
      <disp-formula class="DoCO:FormulaBox" id="F3">
        <label class="DoCO:Label" id="27">3</label>
        <content class="DoCO:Formula" id="28">( p i − q i ) 2 . i =1</content>
      </disp-formula>
      <region class="DoCO:TextChunk" id="29">Despite applying this formula to more than the usual three dimensions, such a formula continues to capture many of the underlying intuitions about what a distance function should capture:</region>
      <region class="DoCO:TextChunk" id="30" confidence="possible">• It is zero if and only if all the measurents for each point are the same. • It obeys the triangle inequality. One consequence of this is that for points p , q and r , we have that D ( p , q ) + D ( q , r ) ≥ D ( p , r ). A more intuitive way to say this is that an intermediate stop along a path cannot make your trip shorter than it would be without the stop. Now, we consider the Euclidean distance as a way to compare images.</region>
      <region class="DoCO:TextChunk" id="34">1.1.3 Euclidean distance for binary images To apply the Euclidean distance to a pair of images, we simply treat each image as a collection of measurements (the pixel values), and apply the Euclidean distance formula of Equation (3). That is, we consider the first pixel in each image to be the first measurement, the second pixel to be the second measurement, and so on. <marker type="page" number="5"/><marker type="block"/> It is worth taking a few moments to think about how to interpret such a function for images. Let’s start with the case of binary images, i.e., images in which each pixel just takes on a value of either 0 or 1. In this case, note that each of the terms under the square root in Equation (3) will take on a 0 or 1, since the squared difference between two binary pixels is either 0 or 1. In particular, the terms will be 1 if the corresponding pixels are different, and 0 if they are the same. Thus, the total sum under the square root will be equal to the number of pixels that are different between the two images. This is a satisfying result, since counting the number of pixels that are different between two binary images seems like at least a reasonable starting point for defining a distance between the images. Note that for the purposes of finding the nearest neighbor , the Euclidean distance is equivalent to the squared Euclidean distance since D ( p , q ) &gt; D ( p , r ) if and only if D ( p , q ) 2 &gt; D ( p , r ) 2 . Many nearest neighbor implementations take advantage of this fact to avoid computing the square root in Equation (3).<marker type="block"/> 1.1.4 Euclidean distance for scalar-valued images and color images If images take on continuous values between 0 and 1, or between say, 0 and 255 for 8-bit images, the principles of Euclidean distance are the same, and we can still use the formula in Equation (3) with one term per pixel. With color images that are represented as RGB images, we now need a mechanism for determining the difference between two pixels, since they are no longer scalar values. Let p r , p g , and p b be the red, green, and blue color channel values of a pixel p , and similarly for a pixel q . Then we can also use the Euclidean distance to define the distance between two color pixel values as</region>
      <outsider class="DoCO:TextBox" type="page_nr" id="32">4</outsider>
      <region class="DoCO:TextChunk" id="35" confidence="possible">D ( p, q ) = ( p r − q r ) 2 + ( p g − q g ) 2 + ( p b − q b ) 2 .</region>
      <region class="DoCO:TextChunk" id="36">If we take this distance between color pixels as a pixel “difference” and plug it into the general formula for Euclidean distance between images (Equation (3)), we obtain n</region>
      <disp-formula class="DoCO:FormulaBox" id="F4">
        <label class="DoCO:Label" id="37">4</label>
        <content class="DoCO:Formula" id="38">D Euc ( p , q ) = ( p i − q i ) 2 i =1 n</content>
      </disp-formula>
      <disp-formula class="DoCO:FormulaBox" id="F5">
        <label class="DoCO:Label" id="39">5</label>
        <content class="DoCO:Formula" id="40">= ( p i r − q i r ) 2 + ( p i g − q i g ) 2 + ( p b i − q i b ) 2 . i =1</content>
      </disp-formula>
      <region class="DoCO:TextChunk" id="41">Note that the expression on the right size of Equation (5) is just another Euclidean distance in which each component of each pixel has been treated as an individual measurement.</region>
      <outsider class="DoCO:TextBox" type="page_nr" id="42">5</outsider>
    </front>
    <body class="DoCO:BodyMatter"/>
  </article>
</pdfx>
