<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>4b983c97320b73f3fe2a71b340189a4cf3e0e03ab8d991e36f72fca8417a126d</job>
    <base_name>10st</base_name>
    <doi>http://dx.doi.org/10.1117/12.2263555.5460687028001</doi>
    <warning>Name identification was not possible. </warning>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <outsider class="DoCO:TextBox" type="header" id="1">Deep Learning Tutorial, Release 0.1</outsider>
      <title-group>
        <article-title class="DoCO:Title" id="2">6.2 Sparse Connectivity</article-title>
      </title-group>
    </front>
    <body class="DoCO:BodyMatter">
      <region class="DoCO:TextChunk" id="14" page="1" column="1">CNNs exploit spatially-local correlation by enforcing a local connectivity pattern between neurons of adjacent layers. In other words, the inputs of hidden units in layer m are from a subset of units in layer m-1 , units that have spatially contiguous receptive fields. We can illustrate this graphically as follows: <marker type="block"/> Imagine that layer m-1 is the input retina. In the above figure, units in layer m have receptive fields of width 3 in the input retina and are thus only connected to 3 adjacent neurons in the retina layer. Units in layer m+1 have a similar connectivity with the layer below. We say that their receptive field with respect to the layer below is also 3, but their receptive field with respect to the input is larger (5). Each unit is unresponsive to variations outside of its receptive field with respect to the retina. The architecture thus ensures that the learnt “filters” produce the strongest response to a spatially local input pattern. However, as shown above, stacking many such layers leads to (non-linear) “filters” that become increasingly “global” (i.e. responsive to a larger region of pixel space). For example, the unit in hidden layer m+1 can encode a non-linear feature of width 5 (in terms of pixel space).<marker type="block"/> In addition, in CNNs, each filter h i is replicated across the entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a feature map .<marker type="block"/> In the above figure, we show 3 hidden units belonging to the same feature map. Weights of the same color are shared—constrained to be identical. Gradient descent can still be used to learn such shared parameters, with only a small change to the original algorithm. The gradient of a shared weight is simply the sum of the gradients of the parameters being shared. Replicating units in this way allows for features to be detected regardless of their position in the visual field. Additionally, weight sharing increases learning efficiency by greatly reducing the number of free parameters being learnt. The constraints on the model enable CNNs to achieve better generalization on vision problems.<marker type="page" number="2"/><marker type="block"/> A feature map is obtained by repeated application of a function across sub-regions of the entire image, in other words, by convolution of the input image with a linear filter, adding a bias term and then applying a non-linear function. If we denote the k-th feature map at a given layer as h k , whose filters are determined by the weights W k and bias b k , then the feature map h k is obtained as follows (for tanh non-linearities):</region>
      <region class="DoCO:FigureBox" id="Fx4">
        <image class="DoCO:Figure" src="10st.page_001.image_01.png" thmb="10st.page_001.image_01-thumb.png"/>
      </region>
      <region class="unknown" id="6" page="1" column="1">6.3 Shared Weights</region>
      <region class="DoCO:FigureBox" id="Fx8">
        <image class="DoCO:Figure" src="10st.page_001.image_02.png" thmb="10st.page_001.image_02-thumb.png"/>
      </region>
      <outsider class="DoCO:TextBox" type="page_nr" id="10" page="1" column="1">52</outsider>
      <outsider class="DoCO:TextBox" type="footer" id="11" page="1" column="1">Chapter 6. Convolutional Neural Networks (LeNet)</outsider>
      <outsider class="DoCO:TextBox" type="header" id="12" page="2" column="1">Deep Learning Tutorial, Release 0.1</outsider>
      <region class="unknown" id="13" page="2" column="1">6.4 Details and Notation</region>
      <region class="DoCO:TextChunk" id="15" confidence="possible" page="2" column="1">h ij k = tanh(( W k ∗ x ) ij + b k ) . Note: Recall the following definition of convolution for a 1D signal. o [ n ] = f [ n ] ∗ g [ n ] = u ∞ = −∞ f [ u ] g [ n − u ] = u ∞ = −∞ f [ n − u ] g [ u ] . This can be extended to 2D as follows: o [ m, n ] = f [ m, n ] ∗ g [ m, n ] = ∞ u = −∞ v ∞ = −∞ f [ u, v ] g [ m − u, n − v ] .</region>
      <region class="DoCO:TextChunk" id="27" page="2" column="1">To form a richer representation of the data, each hidden layer is composed of multiple feature maps, { h ( k ) , k = 0 ..K } . The weights W of a hidden layer can be represented in a 4D tensor containing elements for every combination of destination feature map, source feature map, source vertical position, and source horizontal position. The biases b can be represented as a vector containing one element for every destination feature map. We illustrate this graphically as follows: <marker type="block"/> The figure shows two layers of a CNN. Layer m-1 contains four feature maps. Hidden layer m contains two feature maps ( h 0 and h 1 ). Pixels (neuron outputs) in h 0 and h 1 (outlined as blue and red squares) are computed from pixels of layer (m-1) which fall within their 2x2 receptive field in the layer below (shown as colored rectangles). Notice how the receptive field spans all four input feature maps. The weights W 0 and<marker type="page" number="3"/><marker type="block"/> W 1 of h 0 and h 1 are thus 3D weight tensors. The leading dimension indexes the input feature maps, while the other two refer to the pixel coordinates. Putting it all together, W ij kl denotes the weight connecting each pixel of the k-th feature map at layer m, with the pixel at coordinates (i,j) of the l-th feature map of layer (m-1).<marker type="block"/> ConvOp is the main workhorse for implementing a convolutional layer in Theano. ConvOp is used by theano.tensor.signal.conv2d , which takes two symbolic inputs: • a 4D tensor corresponding to a mini-batch of input images. The shape of the tensor is as follows: [mini-batch size, number of input feature maps, image height, image width]. • a 4D tensor corresponding to the weight matrix W . The shape of the tensor is: [number of feature maps at layer m, number of feature maps at layer m-1, filter height, filter width] Below is the Theano code for implementing a convolutional layer similar to the one of <xref ref-type="fig" rid="F1" id="26" class="deo:Reference">Figure 1</xref>. The input consists of 3 features maps (an RGB color image) of size 120x160. We use two convolutional filters with 9x9 receptive fields.</region>
      <region class="DoCO:FigureBox" id="F6.1">
        <image class="DoCO:Figure" src="10st.page_002.image_03.png" thmb="10st.page_002.image_03-thumb.png"/>
        <caption class="deo:Caption" id="19" page="2" column="1">Figure 6.1: <xref ref-type="fig" rid="F1" id="18" class="deo:Reference">Figure 1</xref> : example of a convolutional layer</caption>
      </region>
      <outsider class="DoCO:TextBox" type="footer" id="21" page="2" column="1">6.4. Details and Notation</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="22" page="2" column="1">53</outsider>
      <outsider class="DoCO:TextBox" type="header" id="23" page="3" column="1">Deep Learning Tutorial, Release 0.1</outsider>
      <region class="unknown" id="25" page="3" column="1">6.5 The Convolution Operator</region>
      <region class="unknown" id="28" page="3" column="1">import theano from theano import tensor as T from theano.tensor.nnet import conv import numpy rng = numpy.random.RandomState(23455) # instantiate 4D tensor for input input = T.tensor4(name=’input’) # initialize shared variable for weights. w_shp = (2, 3, 9, 9) w_bound = numpy.sqrt(3 * 9 * 9) W = theano.shared( numpy.asarray( rng.uniform( low=-1.0 / w_bound, high=1.0 / w_bound, size=w_shp), dtype=input.dtype), name =’W’) # initialize shared variable for bias (1D tensor) with random values # IMPORTANT: biases are usually initialized to zero. However in this # particular application, we simply apply the convolutional layer to # an image without learning the parameters. We therefore initialize # them to random values to "simulate" learning. b_shp = (2,) b = theano.shared(numpy.asarray( rng.uniform(low=-.5, high=.5, size=b_shp), dtype=input.dtype), name =’b’)</region>
      <outsider class="DoCO:TextBox" type="page_nr" id="29" page="3" column="1">54</outsider>
      <outsider class="DoCO:TextBox" type="footer" id="30" page="3" column="1">Chapter 6. Convolutional Neural Networks (LeNet)</outsider>
      <outsider class="DoCO:TextBox" type="header" id="31" page="4" column="1">Deep Learning Tutorial, Release 0.1</outsider>
      <outsider class="DoCO:TextBox" type="sidenote" id="32" page="4" column="1">in w</outsider>
      <region class="unknown" id="33" page="4" column="1"># build symbolic expression that computes the convolution of input with filters conv_out = conv.conv2d(input, W)</region>
      <outsider class="DoCO:TextBox" type="sidenote" id="34" page="4" column="1">neural</outsider>
      <region class="DoCO:TextChunk" id="35" confidence="possible" page="4" column="1"># build symbolic expression to add bias and apply activation function, i.e. produce # A few words on ‘‘dimshuffle‘‘ : # ‘‘dimshuffle‘‘ is a powerful tool in reshaping a tensor; # what it allows you to do is to shuffle dimension around # but also to insert new ones along which the tensor will be # broadcastable; # dimshuffle(’x’, 2, ’x’, 0, 1) # This will work on 3d tensors with no broadcastable # dimensions. The first dimension will be broadcastable, # then we will have the third dimension of the input tensor as # the second of the resulting tensor, etc. If the tensor has # shape (20, 30, 40), the resulting tensor will have dimensions # (1, 40, 1, 20, 30). (AxBxC tensor is mapped to 1xCx1xAxB tensor) # More examples: # dimshuffle(’x’) -&gt; make a 0d (scalar) into a 1d vector # dimshuffle(0, 1) -&gt; identity # dimshuffle(1, 0) -&gt; inverts the first and second dimensions # dimshuffle(’x’, 0) -&gt; make a row out of a 1d vector (N to 1xN) # dimshuffle(0, ’x’) -&gt; make a column out of a 1d vector (N to Nx1) # dimshuffle(2, 0, 1) -&gt; AxBxC to CxAxB # dimshuffle(0, ’x’, 1) -&gt; AxB to Ax1xB # dimshuffle(1, ’x’, 0) -&gt; AxB to Bx1xA output = T.nnet.sigmoid(conv_out + b.dimshuffle(’x’, 0, ’x’, ’x’)) # create theano function to compute filtered images f = theano.function([input], output) Let’s have a little bit of fun with this... import numpy import pylab from PIL import Image # open random image of dimensions 639x516 img = Image.open(open(’doc/images/3wolfmoon.jpg’)) # dimensions are (height, width, channel) img = numpy.asarray(img, dtype=’float64’) / 256. # put image in 4D tensor of shape (1, 3, height, width) img_ = img.transpose(2, 0, 1).reshape(1, 3, 639, 516) filtered_img = f(img_) # plot original image and first and second components of output pylab.subplot(1, 3, 1); pylab.axis(’off’); pylab.imshow(img) pylab.gray(); # recall that the convOp output (filtered image) is actually a "minibatch", # of size 1 here, so we take index 0 in the first dimension: pylab.subplot(1, 3, 2); pylab.axis(’off’); pylab.imshow(filtered_img[0, 0, :, :]) pylab.subplot(1, 3, 3); pylab.axis(’off’); pylab.imshow(filtered_img[0, 1, :, :]) pylab.show()</region>
      <outsider class="DoCO:TextBox" type="footer" id="36" page="4" column="1">6.5. The Convolution Operator</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="37" page="4" column="1">55</outsider>
    </body>
  </article>
</pdfx>