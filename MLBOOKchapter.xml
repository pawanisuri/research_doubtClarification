<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>3ab130cd42cc1316b0cd029c718a7f65887c7801c0e7d629880541d7c712544f</job>
    <base_name>11ev</base_name>
    <doi>http://dx.doi.org/10.21236/ada471251</doi>
    <warning>Name identification was not possible. </warning>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <outsider class="DoCO:TextBox" type="page_nr" id="1">158</outsider>
      <outsider class="DoCO:TextBox" type="header" id="2">CHAPTER 12. EXPLANATION-BASED LEARNING</outsider>
      <title-group>
        <article-title class="DoCO:Title" id="4" confidence="possible">12.2 Domain Theories</article-title>
      </title-group>
    </front>
    <body class="DoCO:BodyMatter">
      <region class="DoCO:TextChunk" id="3" page="1" column="1">the deduction process to establish φ might have been arduous. Rather than have to deduce φ again, we might want to save it, perhaps along with its deduction, in case it is needed later. Shouldn’t that process count as learning? Dietterich [Dietterich, 1990] has called this type of learning speed-up learning. Strictly speaking, speed-up learning does not result in a system being able to make decisions that, in principle, could not have been made before the learning took place. Speed-up learning simply makes it possible to make those decisions more efficiently. But, in practice, this type of learning might make possible certain decisions that might otherwise have been infeasible. To take an extreme case, a chess player can be said to learn chess even though optimal play is inherent in the rules of chess. On the surface, there seems to be no real difference between the experience-based hypotheses that a chess player makes about what constitutes good play and the kind of learning we have been studying so far. As another example, suppose we are given some theorems about geometry and are asked to prove that the sum of the angles of a right triangle is 180 degrees. Let us further suppose that the proof we constructed did not depend on the given triangle being a right triangle; in that case we can learn a more general fact. The learning technique that we are going to study next is related to this example. It is called explanation-based learning (EBL) . EBL can be thought of as a process in which implicit knowledge is converted into explicit knowledge. In EBL, we specialize parts of a domain theory to explain a particular example , then we generalize the explanation to produce another element of the domain theory that will be useful on similar examples. This process is illustrated in Fig. 12.1.</region>
      <region class="DoCO:TextChunk" id="15" page="1" column="1">Two types of information were present in the inductive methods we have studied: the information inherent in the training samples and the information about the domain that is implied by the “bias” (for example, the hypothesis set from which we choose functions). The learning methods are successful only if the hypothesis set is appropriate for the problem. Typically, the smaller the hypothesis set (that is, the more a priori information we have about the function being sought), the less dependent we are on information being supplied by a training set (that is, fewer samples). A priori information about a problem can be expressed in several ways. The methods we have studied so far restrict the hypotheses in a rather direct way. A less direct method involves making assertions in a logical language about the property we are trying to learn. A set of such assertions is usually called a “domain theory.” Suppose, for example, that we wanted to classify people according to whether or not they were good credit risks. We might represent a person by a set of properties (income, marital status, type of employment, etc. ), assemble such <marker type="page" number="2"/><marker type="block"/> data about people who are known to be good and bad credit risks and train a classifier to make decisions. Or, we might go to a loan officer of a bank, ask him or her what sorts of things s/he looks for in making a decision about a loan, encode this knowledge into a set of rules for an expert system, and then use the expert system to make decisions. The knowledge used by the loan officer might have originated as a set of “policies” (the domain theory), but perhaps the application of these policies were specialized and made more efficient through experience with the special cases of loans made in his or her district.<marker type="block"/> To make our discussion more concrete, let’s consider the following fanciful example. We want to find a way to classify robots as “robust” or not. The attributes that we use to represent a robot might include some that are relevant to this decision and some that are not.<marker type="page" number="3"/><marker type="block"/> Suppose we have a domain theory of logical sentences that taken together, help to define whether or not a robot can be classified as robust. (The same domain theory may be useful for several other purposes also, but among other things, it describes the concept “robust.”) In this example, let’s suppose that our domain theory includes the sentences:</region>
      <outsider class="DoCO:TextBox" type="header" id="6" page="2" column="1">12.3. AN EXAMPLE</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="7" page="2" column="1">159</outsider>
      <region class="unknown" id="8" page="2" column="1">Domain Theory specialize Example Prove: X is P (X is P) Complex Proof Process Explanation (Proof) generalize A New Domain Rule: Things "like" X are P Y is like X Trivial Proof Y is P</region>
      <region class="DoCO:FigureBox" id="F12.1">
        <caption class="deo:Caption" id="9" page="2" column="1">Figure 12.1: The EBL Process</caption>
      </region>
      <region class="unknown" id="11" page="2" column="1">12.3 An Example</region>
      <outsider class="DoCO:TextBox" type="page_nr" id="13" page="3" column="1">160</outsider>
      <outsider class="DoCO:TextBox" type="header" id="14" page="3" column="1">CHAPTER 12. EXPLANATION-BASED LEARNING</outsider>
      <region class="DoCO:TextChunk" id="16" confidence="possible" page="3" column="1">F ixes ( u, u ) ⊃ Robust ( u ) (An individual that can fix itself is robust.) Sees ( x, y ) ∧ Habile ( x ) ⊃ F ixes ( x, y ) (A habile individual that can see another entity can fix that entity.) Robot ( w ) ⊃ Sees ( w, w ) (All robots can see themselves.) R 2 D 2( x ) ⊃ Habile ( x ) (R2D2-class individuals are habile.) C 3 P O ( x ) ⊃ Habile ( x ) (C3PO-class individuals are habile.) ...</region>
      <region class="DoCO:TextChunk" id="17" page="3" column="1">(By convention, variables are assumed to be universally quantified.) We could use theorem-proving methods operating on this domain theory to conclude whether certain robots are robust. These methods might be computationally quite expensive because extensive search may have to be performed to derive a conclusion. But after having found a proof for some particular robot, we might be able to derive some new sentence whose use allows a much faster conclusion. We next show how such a new rule might be derived in this example. Suppose we are given a number of facts about Num5, such as:</region>
      <region class="DoCO:TextChunk" id="18" confidence="possible" page="3" column="1">Robot ( N um 5) R 2 D 2( N um 5) Age ( N um 5 , 5) M anuf acturer ( N um 5 , GR ) ...</region>
      <outsider class="DoCO:TextBox" type="header" id="19" page="4" column="1">12.3. AN EXAMPLE</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="20" page="4" column="1">161</outsider>
      <region class="unknown" id="21" page="4" column="1">Robust(Num5) Fixes(u, u) =&gt; Robust(u) Fixes(Num5, Num5) Sees(x,y) &amp; Habile(x) =&gt; Fixes(x,y) Sees(Num5,Num5) Habile(Num5) R2D2(x) Robot(w) =&gt; Habile(x) =&gt; Sees(w,w) Robot(Num5) R2D2(Num5)</region>
      <region class="DoCO:FigureBox" id="F12.2">
        <caption class="deo:Caption" id="22" page="4" column="1">Figure 12.2: A Proof Tree</caption>
      </region>
      <region class="DoCO:TextChunk" id="23" page="4" column="1">We are also told that Robust ( N um 5) is true, but we nevertheless attempt to find a proof of that assertion using these facts about Num5 and the domain theory. The facts about Num5 correspond to the features that we might use to represent Num5. In this example, not all of them are relevant to a decision about Robust ( N um 5). The relevant ones are those used or needed in proving Robust ( N um 5) using the domain theory. The proof tree in Fig. 12.2 is one that a typical theorem-proving system might produce. In the language of EBL, this proof is an explanation for the fact Robust ( N um 5). We see from this explanation that the only facts about Num5 that were used were Robot ( N um 5) and R 2 D 2( N um 5). In fact, we could construct the following rule from this explanation:</region>
      <region class="DoCO:TextChunk" id="24" confidence="possible" page="4" column="1">Robot ( N um 5) ∧ R 2 D 2( N um 5) ⊃ Robust ( N um 5)</region>
      <region class="DoCO:TextChunk" id="46" page="4" column="1">The explanation has allowed us to prune some attributes about Num5 that are irrelevant (at least for deciding Robust ( N um 5)). This type of pruning is the first sense in which an explanation is used to generalize the classification problem. ([DeJong &amp; Mooney, 1986] call this aspect of explanation-based learning feature elimination .) But the rule we extracted from the explanation applies only to Num5. There might be little value in learning that rule since it is so specific. Can it be generalized so that it can be applied to other individuals as well? <marker type="page" number="5"/><marker type="block"/> Examination of the proof shows that the same proof structure, using the same sentences from the domain theory, could be used independently of whether we are talking about Num5 or some other individual. We can generalize the proof by a process that replaces constants in the tip nodes of the proof tree with variables and works upward—using unification to constrain the values of variables as needed to obtain a proof. In this example, we replace Robot ( N um 5) by Robot ( r ) and R 2 D 2( N um 5) by R 2 D 2( s ) and redo the proof—using the explanation proof as a template. Note that we use different values for the two different occurrences of N um 5 at the tip nodes. Doing so sometimes results in more general, but nevertheless valid rules. We now apply the rules used in the proof in the forward direction, keeping track of the substitutions imposed by the most general unifiers used in the proof. (Note that we always substitute terms that are already in the tree for variables in rules.) This process results in the generalized proof tree shown in Fig. 12.3. Note that the occurrence of Sees ( r, r ) as a node in the tree forces the unification of x with y in the domain rule, Sees ( x, y ) ∧ Habile ( y ) ⊃ F ixes ( x, y ). The substitutions are then applied to the variables in the tip nodes and the root node to yield the general rule: Robot ( r ) ∧ R 2 D 2( r ) ⊃ Robust ( r ). This rule is the end result of EBL for this example. The process by which N um 5 in this example was generalized to a variable is what [DeJong &amp; Mooney, 1986] call identity elimination (the precise identity of Num5 turned out to be irrelevant). (The generalization process described in this example is based on that of [DeJong &amp; Mooney, 1986] and differs from that of [Mitchell, et al. , 1986]. It is also similar to that used in [Fikes, et al. , 1972].) Clearly, under certain assumptions, this general rule is more easily used to conclude Robust about an individual than the original proof process was. It is important to note that we could have derived the general rule from the domain theory without using the example. (In the literature, doing so is called static analysis [Etzioni, 1991].) In fact, the example told us nothing new other than what it told us about Num5. The sole role of the example in this instance of EBL was to provide a template for a proof to help guide the generalization process. Basing the generalization process on examples helps to insure that we learn rules matched to the distribution of problems that occur. There are a number of qualifications and elaborations about EBL that need to be mentioned.<marker type="block"/> The domain theory includes a number of predicates other than the one occuring in the formula we are trying to prove and other than those that might custom- arily be used to describe an individual. One might note, for example, that if we used Habile ( N um 5) to describe Num5, the proof would have been shorter. Why didn’t we? The situation is analogous to that of using a data base augmented by logical rules. In the latter application, the formulas in the actual data base<marker type="page" number="6"/><marker type="block"/> are “extensional,” and those in the logical rules are “intensional.” This usage reflects the fact that the predicates in the data base part are defined by their extension—we explicitly list all the tuples sastisfying a relation. The logical rules serve to connect the data base predicates with higher level abstractions that are described (if not defined) by the rules. We typically cannot look up the truth values of formulas containing these intensional predicates; they have to be derived using the rules and the database.<marker type="block"/> The EBL process assumes something similar. The domain theory is useful for connecting formulas that we might want to prove with those whose truth values can be “looked up” or otherwise evaluated. In the EBL literature, such formulas satisfy what is called the operationality criterion . Perhaps another analogy might be to neural networks. The evaluable predicates correspond to the components of the input pattern vector; the predicates in the domain theory correspond to the hidden units. Finding the new rule corresponds to finding a simpler expression for the formula to be proved in terms only of the evaluable predicates.<marker type="page" number="7"/><marker type="block"/> Examining the domain theory for our example reveals that an alternative rule might have been: Robot ( u ) ∧ C 3 P O ( u ) ⊃ Robust ( u ). Such a rule might have resulted if we were given { C 3 P O ( N um 6) , Robot ( N um 6) , . . . } and proved Robust ( N um 6). After considering these two examples (Num5 and Num6), the question arises, do we want to generalize the two rules to something like: Robot ( u ) ∧ [ C 3 P O ( u ) ∨ R 2 D 2( u )] ⊃ Robust ( u )? Doing so is an example of what [DeJong &amp; Mooney, 1986] call structural generalization (via disjunctive augmen- tation ). Adding disjunctions for every alternative proof can soon become cumbersome and destroy any efficiency advantage of EBL. In our example, the efficiency might be retrieved if there were another evaluable predicate, say, Bionic ( u ) such that the domain theory also contained R 2 D 2( x ) ⊃ Bionic ( x ) and C 3 P O ( x ) ⊃ Bionic ( x ). After seeing a number of similar examples, we might be willing to induce the formula Bionic ( u ) ⊃ [ C 3 P O ( u ) ∨ R 2 D 2( u )] in which case the rule with the disjunction could be replaced with Robot ( u ) ∧ Bionic ( u ) ⊃ Robust ( u ).<marker type="block"/> It is well known in theorem proving that the complexity of finding a proof depends both on the number of formulas in the domain theory and on the depth of the shortest proof. Adding a new rule decreases the depth of the shortest proof but it also increases the number of formulas in the domain theory. In realistic applications, the added rules will be relevant for some tasks and not for others. Thus, it is unclear whether the overall utility of the new rules will turn out to be positive. EBL methods have been applied in several settings, usually with positive utility. (See [Minton, 1990] for an analysis).<marker type="block"/> There have been several applications of EBL methods. We mention two here, namely the formation of macro-operators in automatic plan generation and learning how to control search.<marker type="block"/> In automatic planning systems, efficiency can sometimes be enhanced by chain- ing together a sequence of operators into macro-operators . We show an example of a process for creating macro-operators based on techniques explored by [Fikes, et al. , 1972]. Referring to Fig. 12.4, consider the problem of finding a plan for a robot in room R 1 to fetch a box, B 1, by going to an adjacent room, R 2, and pushing it</region>
      <outsider class="DoCO:TextBox" type="page_nr" id="26" page="5" column="1">162</outsider>
      <outsider class="DoCO:TextBox" type="header" id="27" page="5" column="1">EXPLANATION-BASED LEARNING</outsider>
      <region class="unknown" id="29" page="5" column="1">12.4 Evaluable Predicates</region>
      <outsider class="DoCO:TextBox" type="header" id="31" page="6" column="1">12.4. EVALUABLE PREDICATES</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="32" page="6" column="1">163</outsider>
      <region class="unknown" id="33" page="6" column="1">Robust(r) Fixes(u, u) =&gt; Robust(u) {r/u} Fixes(r, r) Sees(x,y) &amp; Habile(x) =&gt; Fixes(x,y) {r/x, r/y, r/s} Sees(r,r) Habile(s) {s/x} R2D2(x) {r/w} Robot(w) =&gt; Habile(x) =&gt; Sees(w,w) Robot(r) R2D2(s) becomes R2D2(r) after applying {r/s}</region>
      <region class="DoCO:FigureBox" id="F12.3">
        <caption class="deo:Caption" id="34" page="6" column="1">Figure 12.3: A Generalized Proof Tree</caption>
      </region>
      <outsider class="DoCO:TextBox" type="page_nr" id="37" page="7" column="1">164</outsider>
      <outsider class="DoCO:TextBox" type="header" id="38" page="7" column="1">CHAPTER 12. EXPLANATION-BASED LEARNING</outsider>
      <region class="unknown" id="39" page="7" column="1">12.5 More General Proofs</region>
      <region class="unknown" id="41" page="7" column="1">12.6 Utility of EBL</region>
      <region class="unknown" id="43" page="7" column="1">12.7 Applications</region>
      <region class="unknown" id="45" page="7" column="1">12.7.1 Macro-Operators in Planning</region>
      <outsider class="DoCO:TextBox" type="header" id="47" page="8" column="1">12.7. APPLICATIONS</outsider>
      <outsider class="DoCO:TextBox" type="page_nr" id="48" page="8" column="1">165</outsider>
      <region class="DoCO:TextChunk" id="49" confidence="possible" page="8" column="1">back to R 1. The goal for the robot is IN ROOM ( B 1 , R 1), and the facts that are true in the initial state are listed in the figure. R1 R2 D1 B1 D2 R3 Initial State: INROOM(ROBOT, R1) INROOM(B1,R2) CONNECTS(D1,R1,R2) CONNECTS(D1,R2,R1) . . .</region>
      <region class="DoCO:FigureBox" id="F12.4">
        <caption class="deo:Caption" id="50" page="8" column="1">Figure 12.4: Initial State of a Robot Problem</caption>
      </region>
      <region class="DoCO:TextChunk" id="51" confidence="possible" page="8" column="1">We will construct the plan from a set of STRIPS operators that include: GOTHRU( d, r 1 , r 2) Preconditions: IN ROOM ( ROBOT, r 1) , CON N ECT S ( d, r 1 , r 2) Delete list: IN ROOM ( ROBOT, r 1) Add list: IN ROOM ( ROBOT, r 2) PUSHTHRU( b, d, r 1 , r 2) Preconditions: IN ROOM ( ROBOT, r 1) , CON N ECT S ( d, r 1 , r 2) , IN ROOM ( b, r 1) Delete list: IN ROOM ( ROBOT, r 1) , IN ROOM ( b, r 1) Add list: IN ROOM ( ROBOT, r 2) , IN ROOM ( b, r 2)</region>
      <region class="DoCO:TextChunk" id="52" page="8" column="1">A backward-reasoning STRIPS system might produce the plan shown in Fig. 12.5. We show there the main goal and the subgoals along a solution path. (The conditions in each subgoal that are true in the initial state are shown underlined.) The preconditions for this plan, true in the initial state, are:</region>
      <region class="DoCO:TextChunk" id="53" confidence="possible" page="8" column="1">IN ROOM ( ROBOT, R 1)</region>
      <outsider class="DoCO:TextBox" type="page_nr" id="54" page="9" column="1">166</outsider>
      <outsider class="DoCO:TextBox" type="header" id="55" page="9" column="1">CHAPTER 12. EXPLANATION-BASED LEARNING</outsider>
      <region class="DoCO:TextChunk" id="56" confidence="possible" page="9" column="1">CON N ECT S ( D 1 , R 1 , R 2) CON N ECT S ( D 1 , R 2 , R 1) IN ROOM ( B 1 , R 2)</region>
      <region class="DoCO:TextChunk" id="57" page="9" column="1">Saving this specific plan, valid only for the specific constants it mentions, would not be as useful as would be saving a more general one. We first generalize these preconditions by substituting variables for constants. We then follow the structure of the specific plan to produce the generalized plan shown in Fig. 12.6 that achieves IN ROOM ( b 1 , r 4). Note that the generalized plan does not require pushing the box back to the place where the robot started. The preconditions for the generalized plan are:</region>
      <region class="DoCO:TextChunk" id="58" confidence="possible" page="9" column="1">IN ROOM ( ROBOT, r 1) CON N ECT S ( d 1 , r 1 , r 2) CON N ECT S ( d 2 , r 2 , r 4) IN ROOM ( b, r 4) INROOM(B1,R1) PUSHTHRU(B1,d,r1,R1) R1 R2 D1 INROOM(ROBOT, r1), INROOM(ROBOT, R2), CONNECTS(d, r1, R1), CONNECTS(D1, R2, R1), INROOM(B1, r1) D1/d} {R2/r1, INROOM(B1, R2) B1 D2 GOTHRU(d2, r3, R2) R3 INROOM(ROBOT, r3), CONNECTS(d2, r3, R2), PLAN: CONNECTS(D1, R2, R1), INROOM(B1, R2) GOTHRU(D1,R1,R2) {R1/r3, D1/d2} PUSHTHRU(B1,D1,R2,R1) INROOM(ROBOT, R1), CONNECTS(D1, R1, R2), CONNECTS(D1, R2, R1), INROOM(B1, R2)</region>
      <region class="DoCO:FigureBox" id="F12.5">
        <caption class="deo:Caption" id="59" page="9" column="1">Figure 12.5: A Plan for the Robot Problem</caption>
      </region>
      <region class="DoCO:TextChunk" id="60" page="9" column="1">Another related technique that chains together sequences of operators to form more general ones is the chunking mechanism in Soar [Laird, et al. , 1986].</region>
    </body>
  </article>
</pdfx>
